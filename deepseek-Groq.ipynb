{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a905f613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.30.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from groq) (4.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\programs\\ai-course\\2- llms\\new_llms\\bert\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Downloading groq-0.30.0-py3-none-any.whl (131 kB)\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.30.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97db8974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, so I need to explain how neural networks work. Hmm, where do I start? I remember that neural networks are inspired by the human brain, but I'm not entirely sure how that translates into computer science. Let me think.\\n\\nFirst, the brain has neurons, right? And each neuron gets signals from other neurons. So maybe in a neural network, there are artificial neurons, or nodes, that process information. I think these nodes are arranged in layers. There's an input layer, some hidden layers, and an output layer. The input layer takes in data, like features from a dataset. Then the hidden layers process that data, and the output layer gives the result.\\n\\nWait, how do the layers communicate? I believe each node in one layer connects to nodes in the next layer through connections called synapses, or in this case, weights. So each connection has a weight that determines the strength of the signal. Then, each node applies an activation function to the weighted sum of its inputs. Activation functions introduce non-linearity, which allows the network to learn complex patterns. Common ones are sigmoid, ReLU, and tanh.\\n\\nBut how does the network learn? I remember something about training the network with data. The process involves forward propagation, where data flows from input to output, and then backward propagation to adjust the weights based on the error. This is done using an optimization algorithm, like gradient descent, which minimizes the loss function. The loss function measures the difference between the network's predictions and the actual targets.\\n\\nOh, and there's something called backpropagation, which efficiently calculates the gradients of the loss with respect to each weight. This is crucial because it allows the network to update the weights in a way that reduces the error.\\n\\nI'm a bit fuzzy on how exactly the weights get updated. I think it's something like this: during forward propagation, the network makes a prediction. Then, the error between the prediction and the actual value is calculated. Using backpropagation, the gradients of this error with respect to each weight are computed. Then, the optimizer adjusts each weight by subtracting the gradient multiplied by a learning rate. This process is repeated for each batch of data, and over multiple epochs, the network learns to make better predictions.\\n\\nWait, what's a batch? I think it's a subset of the training data used in each iteration. So, instead of using the entire dataset at once, which would be computationally expensive, the network processes smaller batches. This is called stochastic gradient descent when the batch size is one, or mini-batch gradient descent for larger batches.\\n\\nAlso, there are different types of neural networks. The most basic one is the feedforward network, where data flows only in one direction. Then there are recurrent neural networks (RNNs) that have feedback connections, allowing them to keep track of sequential information, which is useful for time series data or natural language processing. Convolutional neural networks (CNNs) are another type, which use convolutional layers to process spatial data like images.\\n\\nI should also mention activation functions in more detail. Sigmoid was traditionally used, but it has issues with vanishing gradients. ReLU is more commonly used now because it's computationally efficient and helps avoid the vanishing gradient problem. There are others like tanh, which is similar to sigmoid but scaled, and more recent ones like GELU or Swish.\\n\\nWhat about deep learning? I think that refers to neural networks with many layers, typically more than two. These deep networks can learn hierarchical representations of data, like edges in early layers and complex objects in deeper layers.\\n\\nI'm not sure about the training process in terms of steps. Let me try to outline it:\\n\\n1. **Data Preparation**: The dataset is split into training, validation, and test sets. The features are normalized or standardized.\\n\\n2. **Model Definition**: The architecture of the neural network is defined, including the number of layers, the number of nodes in each layer, activation functions, and the loss function.\\n\\n3. **Forward Propagation**: Input data flows through the network, layer by layer, applying weights and activation functions, to produce an output.\\n\\n4. **Loss Calculation**: The difference between the predicted output and the actual target is calculated using the loss function.\\n\\n5. **Backward Propagation**: Gradients of the loss with respect to each weight are computed using the chain rule.\\n\\n6. **Weight Update**: The optimizer adjusts the weights based on the calculated gradients and the learning rate.\\n\\n7. **Repeat**: Steps 3-6 are repeated for each batch of data, and this process continues for multiple epochs until the model converges or stops improving.\\n\\nI also remember that regularization techniques, like dropout or L2 regularization, are used to prevent overfitting. Dropout randomly deactivates nodes during training to reduce co-adaptation, while L2 regularization adds a penalty term to the loss to keep weights small.\\n\\nAnother important concept is the learning rate. It controls how quickly the weights are updated. A high learning rate might make the model converge faster but could overshoot the minimum, while a low learning rate might lead to slower convergence.\\n\\nI think I've covered the basics. Maybe I should also mention applications to solidify my understanding. Neural networks are used in image classification, speech recognition, natural language processing, game playing, and autonomous vehicles, among other areas.\\n\\nWait, but I'm still a bit confused about how exactly the layers process information. Let me think about it again. Each layer transforms the input data into a more abstract representation. The input layer is just the raw data. The first hidden layer might extract low-level features, the next hidden layer builds on those to form more complex features, and so on. The output layer then uses these high-level features to make predictions or decisions.\\n\\nSo, in a simple example like classifying images, the input layer has pixels, the first hidden layer might detect edges, the next layer detects shapes, and the output layer uses these to determine the object class.\\n\\nI think that's a reasonable way to explain it. Maybe I should also touch on the differences between shallow and deep networks. Shallow networks have fewer layers and can learn simpler patterns, but deep networks can capture more complex and abstract features, making them powerful for tasks like image and speech recognition.\\n\\nI should also mention that training neural networks requires a lot of computational power, especially for deep networks. That's why GPUs are often used, as they can perform parallel computations more efficiently than CPUs.\\n\\nAnother point is that neural networks can be trained in both supervised and unsupervised manners. Supervised learning uses labeled data, where the network learns to map inputs to outputs based on examples. Unsupervised learning deals with unlabeled data, trying to find patterns or intrinsic structures. There's also reinforcement learning, where the network learns through interactions with an environment, receiving rewards or penalties for actions.\\n\\nWait, but reinforcement learning is a bit different. It's more about agents making decisions to maximize cumulative rewards, which is a bit beyond the scope of basic neural networks but still related.\\n\\nI think I've covered the main points. To summarize, neural networks are composed of layers of interconnected nodes that process inputs through weighted connections and activation functions. They learn by adjusting these weights through backpropagation and gradient descent, minimizing the loss function to make accurate predictions or decisions. The architecture and training process can vary based on the task, but the core concepts remain the same.\\n</think>\\n\\nNeural networks are a fundamental concept in machine learning, inspired by the structure and function of the human brain. Here's a structured explanation of how they work:\\n\\n### Architecture of Neural Networks\\n\\n1. **Layers**:\\n   - **Input Layer**: Receives the raw data, such as features from a dataset.\\n   - **Hidden Layers**: Process the data, transforming it into more abstract representations. These layers can be multiple, allowing the network to learn complex patterns.\\n   - **Output Layer**: Generates the final prediction or decision based on the processed data.\\n\\n2. **Nodes (Neurons)**:\\n   - Each node applies an activation function to the weighted sum of its inputs, introducing non-linearity to enable learning of complex patterns. Common activation functions include ReLU, sigmoid, and tanh.\\n\\n3. **Connections**:\\n   - Nodes are connected through edges with weights, which determine the strength of the signal passed between nodes.\\n\\n### Training Process\\n\\n1. **Data Preparation**:\\n   - The dataset is split into training, validation, and test sets. Features are normalized or standardized to ensure effective learning.\\n\\n2. **Forward Propagation**:\\n   - Input data flows through the network layer by layer, applying weights and activation functions to produce an output.\\n\\n3. **Loss Calculation**:\\n   - The difference between the predicted output and the actual target is measured using a loss function, such as mean squared error or cross-entropy.\\n\\n4. **Backward Propagation**:\\n   - Gradients of the loss with respect to each weight are computed using the chain rule, allowing the network to understand how to adjust each weight to minimize the error.\\n\\n5. **Weight Update**:\\n   - An optimizer, such as gradient descent, adjusts the weights based on the calculated gradients and a learning rate, which controls the step size in the direction of the negative gradient.\\n\\n6. **Iteration**:\\n   - The process of forward and backward propagation is repeated for each batch of data, over multiple epochs, until the model converges or stops improving.\\n\\n### Key Concepts\\n\\n- **Activation Functions**: Introduce non-linearity, allowing the network to capture complex patterns. Examples include ReLU, which is efficient and avoids vanishing gradients, and sigmoid for binary outputs.\\n\\n- **Regularization**: Techniques like dropout and L2 regularization prevent overfitting by reducing model complexity.\\n\\n- **Learning Rate**: Controls the step size in weight updates. A high rate can lead to fast convergence but might overshoot minima, while a low rate leads to slower convergence.\\n\\n- **Deep Learning**: Refers to networks with many layers, enabling hierarchical feature learning. These networks are powerful for tasks like image and speech recognition.\\n\\n- **Types of Neural Networks**:\\n  - **Feedforward Networks**: Data flows unidirectionally, suitable for tasks like classification.\\n  - **Recurrent Neural Networks (RNNs)**: Include feedback connections for sequential data processing, such as time series or natural language.\\n  - **Convolutional Neural Networks (CNNs)**: Use convolutional layers for spatial data like images, capturing local features efficiently.\\n\\n### Applications and Considerations\\n\\n- **Applications**: Neural networks are used in image classification, speech recognition, natural language processing, game playing, and autonomous vehicles.\\n\\n- **Computational Requirements**: Training deep networks requires significant computational power, often leveraging GPUs for parallel processing.\\n\\n- **Learning Paradigms**: Can be trained using supervised, unsupervised, or reinforcement learning, each suited for different tasks and data types.\\n\\nIn summary, neural networks learn by adjusting weighted connections through backpropagation and gradient descent, enabling them to make accurate predictions and decisions. Their architecture and training process are tailored to specific tasks, leveraging layers, activation functions, and optimization techniques to achieve complex data processing.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "   messages=[        \n",
    "       {\"role\": \"system\", \"content\": \"You are a professional Data Scientist.\"},\n",
    "       {\"role\": \"user\", \"content\": \"Can you explain how the neural networks work?\"},\n",
    "   ],    \n",
    "   model=\"deepseek-r1-distill-llama-70b\",)\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c5f58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so I need to explain how neural networks work. Hmm, where do I start? I remember that neural networks are inspired by the human brain, but I'm not entirely sure how that translates into computer science. Let me think.\n",
       "\n",
       "First, the brain has neurons, right? And each neuron gets signals from other neurons. So maybe in a neural network, there are artificial neurons, or nodes, that process information. I think these nodes are arranged in layers. There's an input layer, some hidden layers, and an output layer. The input layer takes in data, like features from a dataset. Then the hidden layers process that data, and the output layer gives the result.\n",
       "\n",
       "Wait, how do the layers communicate? I believe each node in one layer connects to nodes in the next layer through connections called synapses, or in this case, weights. So each connection has a weight that determines the strength of the signal. Then, each node applies an activation function to the weighted sum of its inputs. Activation functions introduce non-linearity, which allows the network to learn complex patterns. Common ones are sigmoid, ReLU, and tanh.\n",
       "\n",
       "But how does the network learn? I remember something about training the network with data. The process involves forward propagation, where data flows from input to output, and then backward propagation to adjust the weights based on the error. This is done using an optimization algorithm, like gradient descent, which minimizes the loss function. The loss function measures the difference between the network's predictions and the actual targets.\n",
       "\n",
       "Oh, and there's something called backpropagation, which efficiently calculates the gradients of the loss with respect to each weight. This is crucial because it allows the network to update the weights in a way that reduces the error.\n",
       "\n",
       "I'm a bit fuzzy on how exactly the weights get updated. I think it's something like this: during forward propagation, the network makes a prediction. Then, the error between the prediction and the actual value is calculated. Using backpropagation, the gradients of this error with respect to each weight are computed. Then, the optimizer adjusts each weight by subtracting the gradient multiplied by a learning rate. This process is repeated for each batch of data, and over multiple epochs, the network learns to make better predictions.\n",
       "\n",
       "Wait, what's a batch? I think it's a subset of the training data used in each iteration. So, instead of using the entire dataset at once, which would be computationally expensive, the network processes smaller batches. This is called stochastic gradient descent when the batch size is one, or mini-batch gradient descent for larger batches.\n",
       "\n",
       "Also, there are different types of neural networks. The most basic one is the feedforward network, where data flows only in one direction. Then there are recurrent neural networks (RNNs) that have feedback connections, allowing them to keep track of sequential information, which is useful for time series data or natural language processing. Convolutional neural networks (CNNs) are another type, which use convolutional layers to process spatial data like images.\n",
       "\n",
       "I should also mention activation functions in more detail. Sigmoid was traditionally used, but it has issues with vanishing gradients. ReLU is more commonly used now because it's computationally efficient and helps avoid the vanishing gradient problem. There are others like tanh, which is similar to sigmoid but scaled, and more recent ones like GELU or Swish.\n",
       "\n",
       "What about deep learning? I think that refers to neural networks with many layers, typically more than two. These deep networks can learn hierarchical representations of data, like edges in early layers and complex objects in deeper layers.\n",
       "\n",
       "I'm not sure about the training process in terms of steps. Let me try to outline it:\n",
       "\n",
       "1. **Data Preparation**: The dataset is split into training, validation, and test sets. The features are normalized or standardized.\n",
       "\n",
       "2. **Model Definition**: The architecture of the neural network is defined, including the number of layers, the number of nodes in each layer, activation functions, and the loss function.\n",
       "\n",
       "3. **Forward Propagation**: Input data flows through the network, layer by layer, applying weights and activation functions, to produce an output.\n",
       "\n",
       "4. **Loss Calculation**: The difference between the predicted output and the actual target is calculated using the loss function.\n",
       "\n",
       "5. **Backward Propagation**: Gradients of the loss with respect to each weight are computed using the chain rule.\n",
       "\n",
       "6. **Weight Update**: The optimizer adjusts the weights based on the calculated gradients and the learning rate.\n",
       "\n",
       "7. **Repeat**: Steps 3-6 are repeated for each batch of data, and this process continues for multiple epochs until the model converges or stops improving.\n",
       "\n",
       "I also remember that regularization techniques, like dropout or L2 regularization, are used to prevent overfitting. Dropout randomly deactivates nodes during training to reduce co-adaptation, while L2 regularization adds a penalty term to the loss to keep weights small.\n",
       "\n",
       "Another important concept is the learning rate. It controls how quickly the weights are updated. A high learning rate might make the model converge faster but could overshoot the minimum, while a low learning rate might lead to slower convergence.\n",
       "\n",
       "I think I've covered the basics. Maybe I should also mention applications to solidify my understanding. Neural networks are used in image classification, speech recognition, natural language processing, game playing, and autonomous vehicles, among other areas.\n",
       "\n",
       "Wait, but I'm still a bit confused about how exactly the layers process information. Let me think about it again. Each layer transforms the input data into a more abstract representation. The input layer is just the raw data. The first hidden layer might extract low-level features, the next hidden layer builds on those to form more complex features, and so on. The output layer then uses these high-level features to make predictions or decisions.\n",
       "\n",
       "So, in a simple example like classifying images, the input layer has pixels, the first hidden layer might detect edges, the next layer detects shapes, and the output layer uses these to determine the object class.\n",
       "\n",
       "I think that's a reasonable way to explain it. Maybe I should also touch on the differences between shallow and deep networks. Shallow networks have fewer layers and can learn simpler patterns, but deep networks can capture more complex and abstract features, making them powerful for tasks like image and speech recognition.\n",
       "\n",
       "I should also mention that training neural networks requires a lot of computational power, especially for deep networks. That's why GPUs are often used, as they can perform parallel computations more efficiently than CPUs.\n",
       "\n",
       "Another point is that neural networks can be trained in both supervised and unsupervised manners. Supervised learning uses labeled data, where the network learns to map inputs to outputs based on examples. Unsupervised learning deals with unlabeled data, trying to find patterns or intrinsic structures. There's also reinforcement learning, where the network learns through interactions with an environment, receiving rewards or penalties for actions.\n",
       "\n",
       "Wait, but reinforcement learning is a bit different. It's more about agents making decisions to maximize cumulative rewards, which is a bit beyond the scope of basic neural networks but still related.\n",
       "\n",
       "I think I've covered the main points. To summarize, neural networks are composed of layers of interconnected nodes that process inputs through weighted connections and activation functions. They learn by adjusting these weights through backpropagation and gradient descent, minimizing the loss function to make accurate predictions or decisions. The architecture and training process can vary based on the task, but the core concepts remain the same.\n",
       "</think>\n",
       "\n",
       "Neural networks are a fundamental concept in machine learning, inspired by the structure and function of the human brain. Here's a structured explanation of how they work:\n",
       "\n",
       "### Architecture of Neural Networks\n",
       "\n",
       "1. **Layers**:\n",
       "   - **Input Layer**: Receives the raw data, such as features from a dataset.\n",
       "   - **Hidden Layers**: Process the data, transforming it into more abstract representations. These layers can be multiple, allowing the network to learn complex patterns.\n",
       "   - **Output Layer**: Generates the final prediction or decision based on the processed data.\n",
       "\n",
       "2. **Nodes (Neurons)**:\n",
       "   - Each node applies an activation function to the weighted sum of its inputs, introducing non-linearity to enable learning of complex patterns. Common activation functions include ReLU, sigmoid, and tanh.\n",
       "\n",
       "3. **Connections**:\n",
       "   - Nodes are connected through edges with weights, which determine the strength of the signal passed between nodes.\n",
       "\n",
       "### Training Process\n",
       "\n",
       "1. **Data Preparation**:\n",
       "   - The dataset is split into training, validation, and test sets. Features are normalized or standardized to ensure effective learning.\n",
       "\n",
       "2. **Forward Propagation**:\n",
       "   - Input data flows through the network layer by layer, applying weights and activation functions to produce an output.\n",
       "\n",
       "3. **Loss Calculation**:\n",
       "   - The difference between the predicted output and the actual target is measured using a loss function, such as mean squared error or cross-entropy.\n",
       "\n",
       "4. **Backward Propagation**:\n",
       "   - Gradients of the loss with respect to each weight are computed using the chain rule, allowing the network to understand how to adjust each weight to minimize the error.\n",
       "\n",
       "5. **Weight Update**:\n",
       "   - An optimizer, such as gradient descent, adjusts the weights based on the calculated gradients and a learning rate, which controls the step size in the direction of the negative gradient.\n",
       "\n",
       "6. **Iteration**:\n",
       "   - The process of forward and backward propagation is repeated for each batch of data, over multiple epochs, until the model converges or stops improving.\n",
       "\n",
       "### Key Concepts\n",
       "\n",
       "- **Activation Functions**: Introduce non-linearity, allowing the network to capture complex patterns. Examples include ReLU, which is efficient and avoids vanishing gradients, and sigmoid for binary outputs.\n",
       "\n",
       "- **Regularization**: Techniques like dropout and L2 regularization prevent overfitting by reducing model complexity.\n",
       "\n",
       "- **Learning Rate**: Controls the step size in weight updates. A high rate can lead to fast convergence but might overshoot minima, while a low rate leads to slower convergence.\n",
       "\n",
       "- **Deep Learning**: Refers to networks with many layers, enabling hierarchical feature learning. These networks are powerful for tasks like image and speech recognition.\n",
       "\n",
       "- **Types of Neural Networks**:\n",
       "  - **Feedforward Networks**: Data flows unidirectionally, suitable for tasks like classification.\n",
       "  - **Recurrent Neural Networks (RNNs)**: Include feedback connections for sequential data processing, such as time series or natural language.\n",
       "  - **Convolutional Neural Networks (CNNs)**: Use convolutional layers for spatial data like images, capturing local features efficiently.\n",
       "\n",
       "### Applications and Considerations\n",
       "\n",
       "- **Applications**: Neural networks are used in image classification, speech recognition, natural language processing, game playing, and autonomous vehicles.\n",
       "\n",
       "- **Computational Requirements**: Training deep networks requires significant computational power, often leveraging GPUs for parallel processing.\n",
       "\n",
       "- **Learning Paradigms**: Can be trained using supervised, unsupervised, or reinforcement learning, each suited for different tasks and data types.\n",
       "\n",
       "In summary, neural networks learn by adjusting weighted connections through backpropagation and gradient descent, enabling them to make accurate predictions and decisions. Their architecture and training process are tailored to specific tasks, leveraging layers, activation functions, and optimization techniques to achieve complex data processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d47d167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Alright, the user is asking me to explain how neural networks work. They mentioned they're a data science professional, so I should tailor the explanation to their level.\n",
       "\n",
       "First, I'll start by defining what a neural network is, comparing it to the human brain to make it relatable.\n",
       "\n",
       "Next, I should break down the structure: layers like input, hidden, and output. It's important to explain each layer's role.\n",
       "\n",
       "Then, I need to delve into how data flows through the networkâ€”forward propagation. I'll mention inputs, weights, biases, activation functions, and the output.\n",
       "\n",
       "After that, I should cover the training process. This includes loss functions, backpropagation, and optimization algorithms like gradient descent.\n",
       "\n",
       "Regularization techniques are crucial for preventing overfitting, so I'll list a few common ones like dropout and L1/L2 regularization.\n",
       "\n",
       "I should also explain the different types of neural networks, such as CNNs for images and RNNs for sequences.\n",
       "\n",
       "Finally, I'll touch on the applications and the importance of hyperparameter tuning and evaluation metrics.\n",
       "\n",
       "I need to make sure the explanation is clear but detailed enough for a professional, avoiding overly technical jargon where possible.\n",
       "</think>\n",
       "\n",
       "Ø¨Ø§Ù„Ø·Ø¨Ø¹! Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù‡ÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø±ÙŠØ§Ø¶ÙŠØ© Ù…ØªØ£Ø«Ø±Ø© Ø¨Ø§Ù„Ù‡ÙŠØ¦Ø© ÙˆØ§Ù„ÙˆØ¸ÙŠÙØ© cá»§a Ø§Ù„Ø¯Ù…Ø§Øº Ø§Ù„Ø¨Ø´Ø±ÙŠ. ÙˆÙ‡ÙŠ ØªÙØ³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ù…Ø«Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±ØŒ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠØ©. Ø¥Ù„ÙŠÙƒ Ø´Ø±Ø­ Ù…Ø¨Ø³Ø· Ù„ÙƒÙŠÙÙŠØ© Ø¹Ù…Ù„ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©:\n",
       "\n",
       "### 1. **Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©**\n",
       "Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© ØªØªÙƒÙˆÙ† Ù…Ù† Ø·Ø¨Ù‚Ø§Øª Ù…Ù† Ø§Ù„Ø¹ÙÙ‚Ø¯ (Neurons) Ø§Ù„ØªÙŠ ØªÙØ¹Ø±Ù Ø¨Ø§Ø³Ù… \"Ø§Ù„Ø¹ÙÙ‚Ø¯ Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ©\" (Artificial Neural Units). Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø¹Ø§Ø¯Ø© Ù…Ø§ ØªÙƒÙˆÙ†:\n",
       "- **Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ (Input Layer):** Øª æ¥æ”¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ Ù†Ø±ÙŠØ¯ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§.\n",
       "- **Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø®ÙÙŠØ© (Hidden Layers):** Øª Ø§Ù†Ø¬Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ù…ØªÙˆØ³Ø·Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n",
       "- **Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ (Output Layer):** ØªÙÙ†ØªØ¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©.\n",
       "\n",
       "### 2. **Ø§Ù„Ø¹ÙÙ‚Ø¯ Ø§Ù„Ø¹ØµØ¨ÙŠØ© (Neurons)**\n",
       "ÙƒÙ„ Ø¹Ù‚Ø¯Ø© Ø¹ØµØ¨ÙŠØ© ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Øªæ‰§è¡Œ ÙˆØ¸ÙŠÙØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø³ÙŠØ·Ø©. ØªØªÙ„Ù‚Ù‰ Ø§Ù„Ø¹ÙÙ‚Ø¯ Ø¥Ø´Ø§Ø±Ø§Øª (inputs) Ù…Ù† Ø§Ù„Ø¹ÙÙ‚Ø¯ Ø§Ù„Ø£Ø®Ø±Ù‰ØŒ ÙˆØªØ±Ø¨Ø·Ù‡Ø§ Ø¨Ø£ÙˆØ²Ø§Ù† (weights)ØŒ Ø«Ù… ØªÙØ¶ÙŠÙãƒã‚¤ã‚¢Ø³ (bias) Ø¥Ù„ÙŠÙ‡Ø§ØŒ ÙˆØªØ·Ø¨Ù‚ fonction activation Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¬ Ù„Øªå†³å®š Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØªØ±Ø¬Ø¹ Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¹ÙÙ‚Ø¯ Ø§Ù„ØªØ§Ù„ÙŠØ© Ø£Ùˆ Ù„Ø§.\n",
       "\n",
       "### 3. **Ø§Ù„ØªØ¯ÙÙ‚ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ (Forward Propagation)**\n",
       "Ø¹Ù†Ø¯Ù…Ø§ ØªÙØ¯Ø®Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø´Ø¨ÙƒØ©ØŒ ØªÙ†ØªÙ‚Ù„ Ø¹Ø¨Ø± Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø­ÙˆØ§ÙØ² Ø§Ù„ØªÙŠ ØªØ­Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø¹ÙÙ‚Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø®Ø±Ù‰. ÙÙŠ ÙƒÙ„ Ø®Ø·ÙˆØ©ØŒ ØªÙØ·Ø¨Ù‚ Ø§Ù„Ø¹ÙÙ‚Ø¯ ÙˆØ¸ÙŠÙØ© Ø§Ù„ØªÙ†Ø´ÙŠØ· Ø¹Ù„Ù‰ Ø§Ù„Ù…ixture Ù…Ù† Ø§Ù„Ø¥Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…ÙˆØ²ÙˆÙ†Ø© ÙˆØ§Ù„ãƒã‚¤ã‚¢Ø³ Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬.\n",
       "\n",
       "### 4. **Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Training)**\n",
       "ä¸ºäº† è®­ç»ƒ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©ØŒ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¹Ù„ÙŠÙ…Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…æ ‡è®° (labeled data). ÙŠØªÙ… Ù‡Ø°Ø§ Ù…Ù† Ø®Ù„Ø§Ù„:\n",
       "- **Ø§Ù„Ø®Ø³Ø§Ø±Ø© (Loss Function):** Ù…Ù‚ÙŠØ§Ø³ Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ¹Ù„ÙŠØ© ÙˆØ§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©.\n",
       "- **Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ù…Ø¹Ø§ÙƒØ³ (Backpropagation):** ÙŠØªÙ… ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØªÙŠ Øªå‘ç”Ÿ ÙÙŠ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹ÙÙ‚Ø¯ ÙÙŠ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø®ÙÙŠØ©ØŒ Ù…Ù† Ø®Ù„Ø§Ù„ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø´ØªÙ‚Ø§Øª (gradients) Ø§Ù„ØªÙŠ ØªÙØ¸Ù‡Ø± ÙƒÙŠÙ ÙŠØ¤Ø«Ø± ØªØºÙŠØ± ÙƒÙ„ ÙˆØ²Ù† Ø£Ùˆãƒã‚¤ã‚¢Ø³ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„ÙƒÙ„ÙŠØ©.\n",
       "- **Ø§Ù„ØªØ­Ø³ÙŠÙ† (Optimization):** ØªÙØ³ØªØ®Ø¯Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù…Ø«Ù„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Gradient Descent Ù„Øªè°ƒèŠ‚ Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙˆØ§Ù„ãƒã‚¤ã‚¢Ø³Ø§Øª Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø®Ø³Ø§Ø±Ø©.\n",
       "\n",
       "### 5. **Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø°Ø§ØªÙŠ (Regularization)**\n",
       "ä¸ºäº† Ù…Ù†Ø¹ Ø§Ù„Ø´Ø¨ÙƒØ© Ù…Ù† Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø²Ø§Ø¦Ø¯ (overfitting)ØŒ ØªÙØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø«Ù„:\n",
       "- **Dropout:** ØªÙØºÙ„Ù‚ Ø¨Ø¹Ø¶ Ø§Ù„Ø¹ÙÙ‚Ø¯ Ø¨Ø´ÙƒÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.\n",
       "- **L1/L2 Regularization:** ØªÙØ¶Ø§Ù Ø¬ÙÙˆØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ù„ØªØ­ÙÙŠØ² Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¹Ù„Ù‰ Ø£Ù† ØªÙƒÙˆÙ† ØµØºÙŠØ±Ø©.\n",
       "\n",
       "### 6. **Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©**\n",
       "- **Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø³Ù…ÙŠÙ‘Ø© (Feedforward Networks):** Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ†ØªÙ‚Ù„ ÙÙŠ Ø§ØªØ¬Ø§Ù‡ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·.\n",
       "- **Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© (Recurrent Neural Networks - RNNs):** ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø­Ù„Ù‚Ø§ØªØŒ Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø¹ÙÙ‚Ø¯ Ø£Ù† ØªØ±Ø¬Ø¹ Ø¥Ø´Ø§Ø±Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø®Ù„ÙØŒ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„Ù‡Ø§ Ù…ÙÙŠØ¯Ø© Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ ØªØªØ¶Ù…Ù† ØªØ³Ù„Ø³Ù„Ø§Øª Ø²Ù…Ù†ÙŠØ©.\n",
       "- **Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„ØªÙ„Ø§ÙÙˆÙŠØ© (Convolutional Neural Networks - CNNs):** ØªÙØ³ØªØ®Ø¯Ù… ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª.\n",
       "\n",
       "### 7. **Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª**\n",
       "Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© ØªÙØ³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŒ Ù…Ø«Ù„:\n",
       "- **Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© (Computer Vision):** Ù…Ø«Ù„ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª.\n",
       "- **Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØºØ© (NLP):** Ù…Ø«Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø°ÙƒÙŠØ© ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºÙˆÙŠ.\n",
       "- **Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ©:** Ù…Ø«Ù„ ØªÙˆÙ‚Ø¹ Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø£Ø³Ù‡Ù….\n",
       "\n",
       "### 8. **Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª**\n",
       "- **Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¨Ø·ÙŠØ¡:** Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ÙƒÙ…ÙŠØ§Øª Ù‡Ø§Ø¦Ù„Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù‚ÙˆØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.\n",
       "- **Ø§Ù„ØªÙØ³ÙŠØ±ÙŠØ©:** ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© \"Ø§Ù„ØºØ§Ù…Ø¶Ø©\" (black boxes)ØŒ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„ Ù…Ù† Ø§Ù„ØµØ¹Ø¨ ØªÙØ³ÙŠØ± Ù‚Ø±Ø§Ø±Ø§ØªÙ‡Ø§.\n",
       "- **Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø²Ø§Ø¦Ø¯ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù†Ù‚ØµÙŠ (Overfitting and Underfitting):** ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ù† Øª finds Ø§Ù„ØªÙˆØ§Ø²Ù† Ø§Ù„ØµØ­ÙŠØ­ Ø¨ÙŠÙ† Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ¬Ù†Ø¨ Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ£Ù‚Ù„Ù….\n",
       "\n",
       "### 9. **Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©**\n",
       "- **Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ (Deep Learning):** Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø¨ÙƒØ§Øª Ø¹ØµØ¨ÙŠØ© Ù…Ø¹ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø®ÙÙŠØ©.\n",
       "- **Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ (AutoML):** Ø£Ø¯ÙˆØ§Øª Øªè‡ªåŠ¨ÙŠØ© Ù„ØªØµÙ…ÙŠÙ… ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©.\n",
       "- **Ø§Ù„ØªØ¹Ù„Ù… Ø¨Ø§Ù„ØªØ¹Ø²ÙŠØ² (Reinforcement Learning):** ØªØ¹Ù„Ù… Ø§Ù„Ø´Ø¨ÙƒØ© Ù…Ù† Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ù…Ø­ÙŠØ·Ø©.\n",
       "\n",
       "### 10. **Ø§Ù„ØªÙ‚ÙŠÙŠÙ…**\n",
       "Ø¨Ø¹Ø¯ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ©ØŒ ÙŠØªÙ… ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¦Ù‡Ø§ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± (test data) Ù„Ù‚ÙŠØ§Ø³ Ø¯Ù‚ØªÙ‡Ø§. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…Ø«Ù„Accuracy, Precision, Recall, F1 Score, Ùˆ Mean Squared Error.\n",
       "\n",
       "### Ø§Ù„Ø®Ù„Ø§ØµØ©\n",
       "Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù‡ÙŠ Ø£Ø¯Ø§Ø© Ù‚ÙˆÙŠØ© Ù„ÙÙ‡Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ­Ù‚ÙŠÙ‚ Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª ÙˆØ§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ÙŠØªØ·Ù„Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙ‡Ù…Ù‹Ø§ Ø¬ÙŠØ¯Ù‹Ø§ Ù„Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙˆØ§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø¨Ø±Ø© ÙÙŠ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø´Ø¨ÙƒØ©.\n",
       "\n",
       "Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ Ø£ÙŠ Ø£Ø³Ø¦Ù„Ø© Ø£ÙƒØ«Ø± ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ Ø£Ùˆ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙØ§ØµÙŠÙ„ ØªÙ‚Ù†ÙŠØ© Ø£ÙƒØ«Ø±ØŒ ÙÙ„Ø§ ØªØªØ±Ø¯Ø¯ ÙÙŠ Ø·Ù„Ø¨ Ø°Ù„Ùƒ! ğŸ˜Š"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "   messages=[\n",
    "       {\"role\": \"system\", \"content\": \"Ø£Ù†Øª Ø¹Ø§Ù„Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­ØªØ±Ù.\"},\n",
    "       {\"role\": \"user\", \"content\": \"Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø´Ø±Ø­ ÙƒÙŠÙÙŠØ© Ø¹Ù…Ù„ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©ØŸ\"},\n",
    "   ],    \n",
    "   model=\"deepseek-r1-distill-llama-70b\",)\n",
    "\n",
    "Markdown(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2680f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f4c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6615d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3ca8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
